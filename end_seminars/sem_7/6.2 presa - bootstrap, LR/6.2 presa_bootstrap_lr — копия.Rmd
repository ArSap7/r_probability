---
title: |
  | Гипотезы да АБ-тесты: доля, среднее и дисперсия
date: |
  | 2018-05-24
author: |
  | Филипп
output: 
  revealjs::revealjs_presentation:
    css: left.css
    theme: simple
    highlight: pygments
    center: false
    smart: true
    transition: slide
    background_transition: slide
    reveal_options:
      fig_width: 5
      fig_height: 4
      slideNumber: true
      previewLinks: true
    self_contained: true
    reveal_plugins: []
---


```{r, echo=FALSE, include=FALSE}
library("ggplot2")

```

## Agenda 

- Правда и ничего кроме правды

- Множественная проверка гипотез


## Способы проверить гипотезу

</br>

</br> 

> Какие способы проверить гипотезу вы уже знаете? 


## Способы проверить гипотезу

</br>


> Какие способы проверить гипотезу вы уже знаете? 

- Предположить распределение и использовать параметрический критерий 

- Бутстрап 

## Критерий отношения правдоподобия

</br> 

__Шаг первый:__ оценили модель методом максимального правдоподобия без ограничений и получили $L(\hat \theta_{ML})$

</br>

__Шаг второй:__ оценили модель, наложив ограничение и получили $L(\hat \theta_{0})$

</br>

Оказывается, что 

$$
LR = 2\cdot(\ln L(\hat \theta_{ML}) - \ln L(\theta_0)) \sim \chi^2_q,
$$
где $q$ --- число ограничений

## Пример первый

</br>

Пусть $X_1, \ldots, X_60 \sim iid \hspace{1mm} Pois(\lambda)$ --- количество кино, которое смотрит Женя в неделю. Получилось, что $\sum X_i = 100$. Нужно проверить гипотезу о том, что 

$$
H_0: \lambda = 2 \\
H_A: \lambda \ne 2
$$
</br>

$$ 
lnL = \sum x_i \ln \lambda - n \lambda
$$
</br>
$$
\lambda_{ML} = \bar x = \frac{5}{3}
$$

## Пример первый

</br>

$$
LR = 2\cdot(\ln L(\hat \theta_{ML}) - \ln L(\theta_0)) \sim \chi^2_q,
$$

</br>

```{r, results = 'hold'}

lnL <- function(l){
    return(100*log(l) - 60*l)
  }

2*(lnL(5/3) - lnL(2))
qchisq(0.975, df=1)
```

## Пример второй 

В [табличке](https://yadi.sk/i/hDxMJ0fw3VSDmu) лежит информация о стоимости квартир в Москве и о основных параметрах этих квартир. Пусть цена на квартиры имеет логарифмически нормальное распределение. Оцените параметры $\mu$ и $\sigma$ методом максимального правдоподобия. Постройте для оценок доверительные интервалы. 


```{r, echo = FALSE}
df = read.csv('/Users/fulyankin/Yandex.Disk.localized/R/R_prob_data/flat.csv', sep='\t') 

head(df)
```



## Пример второй 

```{r, results = 'hold'}

x = log(df$price)

lnL <- function(param,data) {
  n <- length(data)
  result <- -n/2*log(2*pi)-n/2*log(param[2]^2)-sum((data-param[1])^2)/(2*param[2]^2)
  return(result)
}

library('maxLik') 

res <- maxLik(lnL, start=c(0.1,0.1),data=x)
summary(res)

```


## Пример второй 

```{r, results = 'hold'}

#lnL_res <- function(sigma,data) lnL(c(5,sigma),x)
#res_restr <- maxLik(lnL_res, start=0.1,data=x)
#summary(res_restr)
#par_restr <- c(5, res_restr$estimate)

#2*(lnL(par, x) - lnL(par_restr, x))
#qchisq(0.975, df=1)
```


## Критерии максимальной мощности 

- В примерах выше мы сравнивали $\frac{L_{ML}}{L_R}$ с единицей

- На основе этого мы делали вывод насколько сильно различаются два правдоподобия и решали что делать с гипотезой

- Сравнивать отношение правдоподобий можно не с единицей, а с какой-то константой $c$

- Лемма Неймана-Пирсона утверждает, что можно подобрать $c$ таким, чтобы у критерий была максимальная мощность (это было на лекции)

- Подробнее смотри в Черновой страницы 86-67


## История про Зомби-лосося

<center>
<img src="losos.jpg" height="600"> 
</center>


## История про Зомби-лосося

- В 2012 году ряд авторов получил шнобелевскую премию по нейробиологии

- Нужно было протестировать аппарат МРТ, обычно для этого берут шарик с маслом и сканируют его 

<center>
<img src="mrt.jpg" height="300"> 
</center>


## История про Зомби-лосося

- Скучно, купили мёртвого лосося

- Стали показывать ему фотки людей

- Задача показать, что в голове лосося нет мозговой активности 

- Аппарат МРТ возвращает кучу данных. Один объект - воксель. Для исследования активности мозга в целом, надо провести множественное тестирование гипотез относительно каждого вокселя.

- Оказалось, что у лосося есть рекация в мозгу на людей, написали [статью](http://prefrontal.org/files/posters/Bennett-Salmon-2009.pdf)


## Проверка гипотезы 

Уровень значимости $P(\text{отвергнуть} H_0 \mid H_0 \text{верна}) = \alpha$ 

Выборка $X_1, \ldots, X_n \sim iid \hspace{1mm} F_X(x, \theta)$

Нулевая гипотеза $H_0 : \theta = \theta_0 $ 

Альтернатива $H_1 : \theta \ne \theta_0 $ 

Статистика $T(X_1, \ldots, X_n)$

Если p-значение $< \alpha$, гипотеза отвергается 


## Две гипотезы 

- Гипотеза сразу о двух параметрах 

- Всё усложняется: можно ошибиться в обеих гипотезах, ошибиться только в первой или только во второй

- Накопление вероятности ошибки первого рода 

$$
P(\text{ошибочно отвергнуть } H_0 ) = \\ = P(\text{ошибочно отвергнуть хотя бы одну из гипотез}) \le \\
\le P(\text{ошибка в первой}) + P(\text{ошибка во второй}) = \\ = \alpha + \alpha = 2\alpha
$$

## Рецепт лечения 

- Будем проверять каждую из двух гипотез на уровне значимости $\frac{\alpha}{2}$

- Если гипотез $K$, то каждую проверяем на уровне $\frac{\alpha}{K}$

- Такой подход называется поправкой Бонферрони


## Разрешение парадокса о лососе

- Если ввести поправку Бонферрони, мёртвый лосось остаётся мёртвым

- Реальный вклад в науку, анализ работ по нйробиологии до 2010 года показал, что окло 40% не используют поправку при множественном тестировании гипотез

- После публикации количество статей, где его не используют упало до 10%

## Другие поправки 

- Поправку Бонферрони просто применять, но у такой процедуры низкая мощность 

- Можно улучшить её, если брать $\alpha_i$ разными 

## Нисходящие методы 

- Отсортируем полученные p-значения 

$$
p_{(1)} \le p_{(2)} \le \ldots \le p_{(m)}
$$
- Если $p_{(1)} \ge \alpha_1$, не отвергаем все нулевые гипотезы, иначе отвергаем $H_{(1)}$ и продолжаем 

- Если $p_{(2)} \ge \alpha_2$, не отвергаем все нулевые гипотезы, иначе отвергаем $H_{(2)}$ и продолжаем 

- Делаем так, пока не закончатся гипотезы

## Метод Холма 

Если взять уровни значимости 

$$\alpha_1 = \frac{\alpha}{m}, \alpha_2 = \frac{\alpha}{m-1}, \ldots, \alpha_i = \frac{\alpha}{m-i+1}, \ldots, \alpha_m = \alpha$$ 
мы получим __метод холма.__ 


## На любой ответ найдётся вопрос 

- Если $X \sim \chi^2_n$ и $Y \sim \chi^2_{n+1}$, $X$ и $Y$ независимы, тогда $X$ всегда меньше $Y$  


## На любой ответ найдётся вопрос 

- График плотности случайной величины, имеющей $t$ распределение симметричен относительно нуля 

## На любой ответ найдётся вопрос 

- Если $X \sim t_n$ тогда $X^2 \sim F_{1,n}$

## На любой ответ найдётся вопрос 

- Если гипотеза отвергается при $5\%$ уровне значимости, то она будет отвергаться и при $1\%$ уровне значимости

- Если гипотеза отвергается при $5\%$ уровне значимости, то она будет отвергаться и при $10\%$ уровне значимости

## На любой ответ найдётся вопрос 

- У $t$-распределения более толстые хвосты, чем у стандартного нормального. Из-за этого случайная величина, имеющая $t$-распределение более предсказуема, чем случайная величина, имеющая нормальное распределение

## На любой ответ найдётся вопрос 

- Если $X \sim N(0,1)$, то $X^2 \sim \chi^2_1$


## На любой ответ найдётся вопрос 

- Мощность теста можно рассчитать заранее, до проведения теста. 

- Точное P-значение можно расчитать заранее, до проведения теста

## На любой ответ найдётся вопрос 

- Мощность больше у того теста, у которого вероятность ошибки первого рода меньше

## На любой ответ найдётся вопрос 

- Оценки метода моментов всегда несмещённые 

- Оценки метода максимального правдоподобия всегда несмещённые 

## На любой ответ найдётся вопрос 

- Из несмещённости оценки всегда следует её состоятельность 

- Если $Var(\hat \theta) \to 0$, тогда оценка состоятельная 


## На любой ответ найдётся вопрос 






